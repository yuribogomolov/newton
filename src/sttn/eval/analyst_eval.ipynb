{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## VARIABLES \"dataset_name\", \"exp_prefix\", \"exp_version\" AND \"model_name\" ##\n",
    "## WILL BE CREATED BY THE nbconvert preprocessor ABOVE                    ##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Upload libraries and initialize env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sttn.nli.analyst import STTNAnalyst\n",
    "from langchain.globals import set_debug\n",
    "from langsmith import Client\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "import backoff\n",
    "import openai\n",
    "\n",
    "from typing import List\n",
    "\n",
    "set_debug(False)\n",
    "\n",
    "# Langsmith client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload `langsmith dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "print(test_dataset.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create evaluators and make `STTNAnalyst` results compatible to `langsmith.evaluator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap `STTNAnalyst.chat` results in a compatible function with backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, (openai.RateLimitError), max_tries=6, base=8, factor=1, max_value=60)\n",
    "def get_context_with_backoff(model_name, inputs):\n",
    "    return STTNAnalyst(model_name=model_name).chat(user_query=inputs[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst_results(inputs: dict, model_name=\"gpt-4o\") -> dict:\n",
    "    try:\n",
    "        context = get_context_with_backoff(inputs=inputs, model_name=model_name)\n",
    "    except Exception as e:\n",
    "        print(f'An error happened while launching Analyst on query: \\n\"{inputs[\"question\"]}\"\\n', str(e))\n",
    "        return {\"data_provider_id\": \"\",\n",
    "                \"data_provider_args\": {},\n",
    "                \"result\": None,\n",
    "                \"executable\": False}\n",
    "    \n",
    "    # Get the data provider id and args from the context\n",
    "    data_provider_id = context.data_provider_id if context.data_provider_id else ''\n",
    "    data_provider_args = context.data_provider_args if context.data_provider_args else {}\n",
    "    result = context.result\n",
    "    analysis_code = context.analysis_code\n",
    "    \n",
    "    return {\"data_provider_id\": data_provider_id,\n",
    "            \"data_provider_args\": data_provider_args,\n",
    "            \"result\": result,\n",
    "            \"executable\": True,\n",
    "            #\"analysis_code\": analysis_code\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluators for `data_provider_id`,  `data_provider_args` and `executable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_provider_id_match(run: Run, example: Example) -> dict:\n",
    "    ref_provider_id = example.outputs[\"data_provider_id\"]\n",
    "    pred_provider_id = run.outputs[\"data_provider_id\"]\n",
    "    score = pred_provider_id == ref_provider_id\n",
    "    return {\"key\": \"data_provider_match\",\n",
    "            \"score\": int(score)}\n",
    "\n",
    "def data_provider_args_match(run: Run, example: Example) -> dict:\n",
    "    ref_provider_args = example.outputs[\"data_provider_args\"]\n",
    "    pred_provider_args = run.outputs[\"data_provider_args\"]\n",
    "    score = pred_provider_args == ref_provider_args\n",
    "    return {\"key\": \"data_provider_args_match\",\n",
    "            \"score\": int(score)}\n",
    "\n",
    "def result_match(run: Run, example: Example) -> dict:\n",
    "    try:\n",
    "        ref_result = example.outputs[\"result\"]\n",
    "        pred_result = run.outputs[\"result\"]\n",
    "        score = pred_result == ref_result\n",
    "        return {\"key\": \"result_match\",\n",
    "                \"score\": int(score)}\n",
    "    except Exception as e:\n",
    "        print(f'An error happened while comparing the results: {str(e)}')\n",
    "        print(\"Result ATTRIBUTE MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"result_match\",\n",
    "                \"score\": -1}\n",
    "\n",
    "def executable_match(run: Run, example: Example) -> dict:\n",
    "    ref_provider_args = example.outputs[\"executable\"]\n",
    "    pred_provider_args = run.outputs[\"executable\"]\n",
    "    score = pred_provider_args == ref_provider_args\n",
    "    return {\"key\": \"executable\",\n",
    "            \"score\": bool(score)}\n",
    "\n",
    "analysis_code_score_eval = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\", \n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"How accurate is this code compared to the reference on a scale of 1-10?\",\n",
    "            \"efficiency\": \"How efficient is this code compared to the reference on a scale of 1-10?\",  # can change to bool\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"analysis_code\"], \n",
    "        \"reference\": example.outputs[\"analysis_code\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary evaluators for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific data_provider_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi\n",
    "def taxi_dp_id_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluator for accuracy score only for NycTaxiDataProvider.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"taxi_dp_id_accuracy\",\n",
    "             \"score\": float} - the accuracy score of the correct matches of NycTaxiDataProvider\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sum_id_match = 0\n",
    "        taxi_examples = 0\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"data_provider_id\"] == \"NycTaxiDataProvider\":\n",
    "                sum_id_match += data_provider_id_match(run, example)['score']\n",
    "                taxi_examples += 1\n",
    "        \n",
    "        return {\"key\": \"taxi_dp_id_accuracy\",\n",
    "                \"score\": sum_id_match/taxi_examples}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating NycTaxiDataProvider accuracy: {str(e)}\")\n",
    "        print(\"NycTaxiDataProvider MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"taxi_dp_id_accuracy\",\n",
    "                \"score\": -1.0}\n",
    "\n",
    "\n",
    "# LEHD\n",
    "def lehd_dp_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluator for id accuracy only for OriginDestinationEmploymentDataProvider data provider.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"lehd_dp_id_accuracy\",\n",
    "             \"score\": float} - the accuracy score of the correct matches of OriginDestinationEmploymentDataProvider\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sum_id_match = 0\n",
    "        lehd_examples = 0\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"data_provider_id\"] == \"OriginDestinationEmploymentDataProvider\":\n",
    "                sum_id_match += data_provider_id_match(run, example)['score']\n",
    "                lehd_examples += 1\n",
    "        \n",
    "        return {\"key\": \"lehd_dp_id_accuracy\",\n",
    "                \"score\": sum_id_match/lehd_examples}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating LEHD data provider accuracy: {str(e)}\")\n",
    "        print(\"OriginDestinationEmploymentDataProvider MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"lehd_dp_id_accuracy\",\n",
    "                \"score\": -1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific data_provider_args (when id predicted correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi\n",
    "def taxi_dp_args_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluator for args accuracy only when NycTaxiDataProvider was correctly picked.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"taxi_dp_args_accuracy\",\n",
    "             \"score\": float} - the accuracy score for the args of NycTaxiDataProvider\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sum_id_match = 0\n",
    "        sum_args_match = 0\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"data_provider_id\"] == \"NycTaxiDataProvider\":\n",
    "                id_match = data_provider_id_match(run, example)['score']\n",
    "                sum_id_match += id_match\n",
    "\n",
    "                # check the args only when id was predicted correctly            \n",
    "                if id_match:\n",
    "                    args_match = data_provider_args_match(run, example)['score']\n",
    "                    sum_args_match += args_match\n",
    "    \n",
    "        return {\"key\": \"taxi_dp_args_accuracy\",\n",
    "                \"score\": sum_args_match/sum_id_match}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating NycTaxiDataProvider args accuracy: {str(e)}\")\n",
    "        print(\"NycTaxiDataProvider MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"taxi_dp_args_accuracy\",\n",
    "                \"score\": -1.0}\n",
    "\n",
    "\n",
    "# LEHD\n",
    "def lehd_dp_args_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\" \n",
    "    Evaluator for args accuracy only when OriginDestinationEmploymentDataProvider was correctly picked.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"lehd_dp_args_accuracy\",\n",
    "             \"score\": float} - the accuracy score for the args of OriginDestinationEmploymentDataProvider\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sum_id_match = 0\n",
    "        sum_args_match = 0\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"data_provider_id\"] == \"OriginDestinationEmploymentDataProvider\":\n",
    "                id_match = data_provider_id_match(run, example)['score']\n",
    "                sum_id_match += id_match\n",
    "\n",
    "                # check the args only when id was predicted correctly\n",
    "                if id_match:\n",
    "                    args_match = data_provider_args_match(run, example)['score']\n",
    "                    sum_args_match += args_match\n",
    "        \n",
    "        return {\"key\": \"lehd_dp_args_accuracy\",\n",
    "                \"score\": sum_args_match/sum_id_match}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating LEHD data provider args accuracy: {str(e)}\")\n",
    "        print(\"OriginDestinationEmploymentDataProvider MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"lehd_dp_args_accuracy\",\n",
    "                \"score\": -1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low complexity\n",
    "def low_complexity_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Accuracy evaluator for the low complexity queries.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"low_compl_accuracy\",\n",
    "             \"score\": float} - the accuracy score for the low complexity queries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        low_comp_correct_id_and_args = 0\n",
    "        low_comp_examples = 0\n",
    "\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"complexity\"] == \"low\":\n",
    "                id_match = data_provider_id_match(run, example)['score']\n",
    "                low_comp_examples += 1\n",
    "                if id_match:\n",
    "                    args_match = data_provider_args_match(run, example)['score']\n",
    "                    low_comp_correct_id_and_args += args_match\n",
    "        \n",
    "        return {\"key\": \"low_compl_accuracy\",\n",
    "                \"score\": low_comp_correct_id_and_args/low_comp_examples}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating low complexity accuracy: {str(e)}\")\n",
    "        print(\"LOW COMPLEXITY FEATURE MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"low_compl_accuracy\",\n",
    "                \"score\": -1.0}\n",
    "\n",
    "\n",
    "# Medium complexity\n",
    "def medium_complexity_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Accuracy evaluator for the medium complexity queries.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"medium_compl_accuracy\",\n",
    "             \"score\": float} - the accuracy score for the medium complexity queries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        med_comp_correct_id_and_args = 0\n",
    "        med_comp_examples = 0\n",
    "\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"complexity\"] == \"medium\":\n",
    "                id_match = data_provider_id_match(run, example)['score']\n",
    "                med_comp_examples += 1\n",
    "                if id_match:\n",
    "                    args_match = data_provider_args_match(run, example)['score']\n",
    "                    med_comp_correct_id_and_args += args_match\n",
    "        \n",
    "        return {\"key\": \"medium_compl_accuracy\",\n",
    "                \"score\": med_comp_correct_id_and_args/med_comp_examples}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating medium complexity accuracy: {str(e)}\")\n",
    "        print(\"MEDIUM COMPLEXITY FEATURE MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"medium_compl_accuracy\",\n",
    "                \"score\": -1.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad queries (bad grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_grammar_accuracy_summary_eval(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Accuracy evaluator for the bad grammar queries.\n",
    "    ### Parameters:\n",
    "    - runs: List[Run] - list of LangChain Run objects\n",
    "    - examples: List[Example] - list of Langsmith Example objects (from the test dataset)\n",
    "\n",
    "    ### Returns:\n",
    "    - dict: {\"key\": \"bad_grammar_accuracy\",\n",
    "             \"score\": float} - the accuracy score for the bad grammar queries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bad_grammar_correct_id_and_args = 0\n",
    "        bad_grammar_examples = 0\n",
    "\n",
    "        for run, example in zip(runs, examples):\n",
    "            if example.outputs[\"bad_grammar\"]:\n",
    "                id_match = data_provider_id_match(run, example)['score']\n",
    "                bad_grammar_examples += 1\n",
    "                if id_match:\n",
    "                    args_match = data_provider_args_match(run, example)['score']\n",
    "                    bad_grammar_correct_id_and_args += args_match\n",
    "        \n",
    "        return {\"key\": \"bad_grammar_accuracy\",\n",
    "                \"score\": bad_grammar_correct_id_and_args/bad_grammar_examples}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while evaluating bad grammar accuracy: {str(e)}\")\n",
    "        print(\"BAD_GRAMMAR FEATURE MIGHT BE MISSING IN THE TEST DATASET\")\n",
    "        return {\"key\": \"bad_grammar_accuracy\",\n",
    "                \"score\": -1.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = STTNAnalyst(model_name=model_name, verbose=False)  # model_name is initialized by runner script at the begginning of the notebook\n",
    "\n",
    "chain_results = evaluate(\n",
    "    analyst_results,  #  AI system (wrapped function with outputs as dict),\n",
    "    data=test_dataset.name,  # The dataset name to predict and grade over\n",
    "    evaluators=[\n",
    "        data_provider_id_match,\n",
    "        data_provider_args_match,\n",
    "        executable_match,\n",
    "        result_match,\n",
    "        #analysis_code_score_eval\n",
    "        ],  # The evaluators to score the results\n",
    "    summary_evaluators=[\n",
    "        taxi_dp_id_accuracy_summary_eval,\n",
    "        taxi_dp_args_accuracy_summary_eval,\n",
    "        lehd_dp_accuracy_summary_eval,\n",
    "        lehd_dp_args_accuracy_summary_eval,\n",
    "        low_complexity_accuracy_summary_eval,\n",
    "        medium_complexity_accuracy_summary_eval,\n",
    "        bad_grammar_accuracy_summary_eval,\n",
    "        ],  # summary evluators to score the overall results\n",
    "    experiment_prefix=exp_prefix,  # A prefix for your experiment names to easily identify them\n",
    "    metadata={\n",
    "      \"version\": f\"{exp_version}\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
